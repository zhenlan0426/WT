{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization, Activation,Input,Lambda\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import mean_absolute_error\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: relu at output\n",
    "# TODO: repeat X\n",
    "# TODO: BN train mode\n",
    "# TODO: remove baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 2000\n",
    "dateRange = (370,670)\n",
    "weight = 1/np.arange(1000,700,-1)\n",
    "weight = weight/np.sum(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1  = pd.read_csv('train_2.csv')\n",
    "missing = train1.isnull()\n",
    "nonMissingIndex = np.logical_not(missing).values\n",
    "train1 = train1.fillna(0)\n",
    "train1.iloc[:,1:] = train1.iloc[:,1:].astype(np.float32)\n",
    "n = train1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train1.Page.str.split('mediawiki.org|wikimedia.org|wikipedia.org',expand=True)\n",
    "b = np.where(a.loc[:,2].isnull(),a.loc[:,1],a.loc[:,2])\n",
    "_,index_array = np.unique(b,return_inverse=True) # access & agent\n",
    "website_array = [train1.Page.str.contains(web).values.astype(np.int8) \\\n",
    "                 for web in ['mediawiki.org','wikimedia.org','wikipedia.org']] # website\n",
    "lang_array = train1.Page.str.extract('([a-z]{2}).wikipedia').fillna('nan').values.flatten()\n",
    "_,lang_array = np.unique(lang_array,return_inverse=True) # language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Page_X = \\\n",
    "np.concatenate([OneHotEncoder(dtype=np.float32,sparse=False).fit_transform(lang_array[:,np.newaxis]),\\\n",
    "                OneHotEncoder(dtype=np.float32,sparse=False).fit_transform(index_array[:,np.newaxis]),\\\n",
    "                np.stack(website_array,1)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index=pd.to_datetime(train1.columns[1:]).to_series().reset_index(drop=True)\n",
    "\n",
    "'''def extract_date_info(timestamp,mss):\n",
    "    return np.array([mss, timestamp.day, \\\n",
    "                     timestamp.dayofyear]+\\\n",
    "                    [1 if i==timestamp.year else 0 for i in [2015, 2016, 2017]]+\\\n",
    "                    [1 if i==timestamp.weekday() else 0 for i in range(7)]+\\\n",
    "                    [1 if i==timestamp.month else 0 for i in range(12)])'''\n",
    "\n",
    "def extract_date_info(timestamp,mss):\n",
    "    return np.array([mss, timestamp.day, \\\n",
    "                     timestamp.dayofyear,\\\n",
    "                     timestamp.year,\\\n",
    "                     timestamp.month,\\\n",
    "                     timestamp.is_month_start,\\\n",
    "                     timestamp.is_month_end]+\\\n",
    "                    [1 if i==timestamp.weekday() else 0 for i in range(7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = extract_date_info(date_index[3],3).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = lambda df,start,interval: np.max(df.iloc[:,start-interval:start].values,1)\n",
    "min_ = lambda df,start,interval: np.min(df.iloc[:,start-interval:start].values,1)\n",
    "std_ = lambda df,start,interval: np.std(df.iloc[:,start-interval:start].values,1)\n",
    "mean_ = lambda df,start,interval: np.mean(df.iloc[:,start-interval:start].values,1)\n",
    "growth_ = lambda df,start,interval: np.mean(df.iloc[:,start-interval:start],1) -\\\n",
    "                                    np.mean(df.iloc[:,start-2*interval:start-interval],1)\n",
    "\n",
    "fun_list = [max_,min_,std_,mean_,growth_]\n",
    "interval_list = [14,30,60]\n",
    "T = train1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(batchSize,dateRange,weight):\n",
    "    while True:\n",
    "        j = np.random.choice(range(*dateRange),p=weight)\n",
    "        i = np.random.choice(range(j+2,j+64))\n",
    "        index_ = np.random.choice(np.where(nonMissingIndex[:,i])[0],size=batchSize)\n",
    "        temp = train1.iloc[index_]\n",
    "        Lookback_X = np.stack([fun(temp,j,interval) for fun in fun_list for interval in interval_list],1)\n",
    "        yield np.concatenate([Page_X[index_],\\\n",
    "                                Lookback_X,\\\n",
    "                                np.broadcast_to(extract_date_info(date_index[i-1],i-j),(batchSize,d)),\\\n",
    "                                temp.iloc[:,j-1:j].values,\\\n",
    "                                temp.iloc[:,i-90:i-89].values,\\\n",
    "                                temp.iloc[:,i-120:i-119].values,\\\n",
    "                                temp.iloc[:,i-180:i-179].values,\\\n",
    "                                temp.iloc[:,i-365:i-364].values],1).astype(np.float32),\\\n",
    "               temp.iloc[:,j-1].values[:,np.newaxis],\\\n",
    "               temp.iloc[:,i].values[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.concatenate([next(Generator(batchSize,dateRange,weight))[0] for _ in range(2000)],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_ = temp.std(0)\n",
    "mean_ = temp.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('std_.txt',std_)\n",
    "# np.savetxt('mean_.txt',mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(batchSize,dateRange,weight):\n",
    "    while True:\n",
    "        j = np.random.choice(range(*dateRange),p=weight)\n",
    "        i = np.random.choice(range(j+2,j+64))\n",
    "        index_ = np.random.choice(np.where(nonMissingIndex[:,i])[0],size=batchSize)\n",
    "        temp = train1.iloc[index_]\n",
    "        Lookback_X = np.stack([fun(temp,j,interval) for fun in fun_list for interval in interval_list],1)\n",
    "        yield ([(np.concatenate([Page_X[index_],\\\n",
    "                                Lookback_X,\\\n",
    "                                np.broadcast_to(extract_date_info(date_index[i-1],i-j),(batchSize,d)),\\\n",
    "                                temp.iloc[:,j-1:j].values,\\\n",
    "                                temp.iloc[:,i-90:i-89].values,\\\n",
    "                                temp.iloc[:,i-120:i-119].values,\\\n",
    "                                temp.iloc[:,i-180:i-179].values,\\\n",
    "                                temp.iloc[:,i-365:i-364].values],1).astype(np.float32) - mean_)/std_,\\\n",
    "                temp.iloc[:,j-1].values[:,np.newaxis]],\\\n",
    "                temp.iloc[:,i].values[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator_noBL(batchSize,dateRange,weight):\n",
    "    while True:\n",
    "        j = np.random.choice(range(*dateRange),p=weight)\n",
    "        i = np.random.choice(range(j+2,j+64))\n",
    "        index_ = np.random.choice(np.where(nonMissingIndex[:,i])[0],size=batchSize)\n",
    "        temp = train1.iloc[index_]\n",
    "        Lookback_X = np.stack([fun(temp,j,interval) for fun in fun_list for interval in interval_list],1)\n",
    "        yield ((np.concatenate([Page_X[index_],\\\n",
    "                                Lookback_X,\\\n",
    "                                np.broadcast_to(extract_date_info(date_index[i-1],i-j),(batchSize,d)),\\\n",
    "                                temp.iloc[:,j-1:j].values,\\\n",
    "                                temp.iloc[:,i-90:i-89].values,\\\n",
    "                                temp.iloc[:,i-120:i-119].values,\\\n",
    "                                temp.iloc[:,i-180:i-179].values,\\\n",
    "                                temp.iloc[:,i-365:i-364].values],1).astype(np.float32) - mean_)/std_,\\\n",
    "                temp.iloc[:,i].values[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "downSampleRate = 0.25\n",
    "j = T - 64\n",
    "Lookback_X = np.stack([fun(train1,j,interval) for fun in fun_list for interval in interval_list],1)\n",
    "testData = []\n",
    "testMargain = []\n",
    "for i in range(j+2,j+64):\n",
    "    temp = np.concatenate([train1.iloc[:,i].values[:,np.newaxis],\\\n",
    "                           Page_X,\\\n",
    "                           np.broadcast_to(extract_date_info(date_index[i-1],i-j),(n,d)),\\\n",
    "                           Lookback_X,\\\n",
    "                           train1.iloc[:,j-1:j].values,\\\n",
    "                           train1.iloc[:,i-90:i-89].values,\\\n",
    "                           train1.iloc[:,i-120:i-119].values,\\\n",
    "                           train1.iloc[:,i-180:i-179].values,\\\n",
    "                           train1.iloc[:,i-365:i-364].values],1).astype(np.float32)\n",
    "    nonmissing_index = np.logical_not(missing.iloc[:,i].values)\n",
    "    random_index = np.random.rand(nonmissing_index.sum())<downSampleRate\n",
    "    testData.append(temp[nonmissing_index][random_index])\n",
    "    testMargain.append(train1.iloc[:,j-1].values[nonmissing_index][random_index])\n",
    "testData = np.concatenate(testData,0)\n",
    "testMargain = np.concatenate(testMargain,0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = testData[:,0][:,np.newaxis]\n",
    "testX = testData[:,1:]\n",
    "testMargain = testMargain[:,np.newaxis]\n",
    "del testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = (testX-mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 2\n",
    "d_X = 200\n",
    "L2 = 1e-2\n",
    "learningR = 1e-3\n",
    "eps = 0.1\n",
    "epoch = 500\n",
    "monitor = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = Input(shape=(testX.shape[1],))\n",
    "base_tf = Input(shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tf = X_tf\n",
    "for _ in range(depth):   \n",
    "    temp_tf = Dense(d_X,kernel_regularizer=regularizers.l2(L2))(temp_tf)\n",
    "    temp_tf = BatchNormalization()(temp_tf)\n",
    "    #temp_tf = keras.layers.LeakyReLU()(temp_tf)\n",
    "    temp_tf = keras.layers.PReLU()(temp_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tf = Dense(1,kernel_regularizer=regularizers.l2(L2))(temp_tf)\n",
    "yhat_tf = Lambda(lambda x:x[0]+x[1])([temp_tf,base_tf])\n",
    "#yhat_tf = Dense(1,kernel_regularizer=regularizers.l2(L2))(temp_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model([X_tf,base_tf],yhat_tf)\n",
    "#model1 = Model(X_tf,yhat_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE_tf(y, yhat):\n",
    "    summ = tf.abs(y) + tf.abs(yhat) + 1e-3\n",
    "    return 200*tf.reduce_mean(tf.abs(y - yhat)/summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE_train(y, yhat):\n",
    "    summ = tf.abs(y) + tf.abs(yhat) + 1e-3\n",
    "    return 200*tf.reduce_mean(tf.abs(y - yhat)/summ,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE_np(y, yhat):\n",
    "    summ = np.abs(y) + np.abs(yhat) + 1e-3\n",
    "    return 200*np.mean(np.abs(y - yhat)/summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.compile(keras.optimizers.SGD(lr=learningR),keras.losses.MAE,[SMAPE_tf])\n",
    "model1.compile(keras.optimizers.Adam(lr=learningR),SMAPE_train,[SMAPE_tf])\n",
    "#model1.compile(keras.optimizers.SGD(lr=learningR),SMAPE_train,[SMAPE_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 53.8141 - SMAPE_tf: 51.4777 - val_loss: 63.4331 - val_SMAPE_tf: 61.4671\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 52.7499 - SMAPE_tf: 51.0083 - val_loss: 50.2388 - val_SMAPE_tf: 48.6949\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 51.9566 - SMAPE_tf: 50.5619 - val_loss: 48.3480 - val_SMAPE_tf: 47.0883\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 52.8319 - SMAPE_tf: 51.6829 - val_loss: 47.9964 - val_SMAPE_tf: 46.9517\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 52.4825 - SMAPE_tf: 51.5248 - val_loss: 47.8038 - val_SMAPE_tf: 46.9311\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 51.0700 - SMAPE_tf: 50.2695 - val_loss: 47.6383 - val_SMAPE_tf: 46.9059\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 50.8483 - SMAPE_tf: 50.1713 - val_loss: 47.8662 - val_SMAPE_tf: 47.2423\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 51.8295 - SMAPE_tf: 51.2502 - val_loss: 47.4997 - val_SMAPE_tf: 46.9629\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 52.0294 - SMAPE_tf: 51.5288 - val_loss: 47.4476 - val_SMAPE_tf: 46.9837\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 52.3205 - SMAPE_tf: 51.8885 - val_loss: 47.3239 - val_SMAPE_tf: 46.9232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc11bf61390>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(Generator(batchSize,dateRange,weight),100,10,1,validation_data=([testX,testMargain],testY))\n",
    "#model1.fit_generator(Generator_noBL(batchSize,dateRange,weight),100,10,1,validation_data=(testX,testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = date_index.append(pd.Series(pd.date_range('2017-09-11','2017-11-13')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = date_index.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = T\n",
    "Lookback_X = np.stack([fun(train1,j,interval) for fun in fun_list for interval in interval_list],1)\n",
    "testData = []\n",
    "testMargain = []\n",
    "mapping_index = []\n",
    "for i in range(j+2,j+64):\n",
    "    temp = np.concatenate([Page_X,\\\n",
    "                           np.broadcast_to(extract_date_info(date_index[i-1],i-j),(n,d)),\\\n",
    "                           Lookback_X,\\\n",
    "                           train1.iloc[:,j-1:j].values,\\\n",
    "                           train1.iloc[:,i-90:i-89].values,\\\n",
    "                           train1.iloc[:,i-120:i-119].values,\\\n",
    "                           train1.iloc[:,i-180:i-179].values,\\\n",
    "                           train1.iloc[:,i-365:i-364].values],1).astype(np.float32)\n",
    "\n",
    "    testData.append(temp)\n",
    "    testMargain.append(train1.iloc[:,j-1].values)\n",
    "    mapping_index.append(train1.iloc[:,0].str.cat([str(date_index[i-1])[:10]]*n,'_'))\n",
    "    \n",
    "testData = np.concatenate(testData,0)\n",
    "testMargain = np.concatenate(testMargain,0)\n",
    "mapping_index = np.concatenate(mapping_index,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data corruption when creating a a DMatrix with label being a non-contiguous ndarray #2554\n",
    "dtest = xgb.DMatrix(np.ascontiguousarray(testData))\n",
    "dtest.set_base_margin(np.ascontiguousarray(testMargain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.maximum(model_gbm.predict(dtest,output_margin=True),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = pd.DataFrame({'Page':mapping_index,'Visits':yhat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = pd.read_csv('key_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = key.merge(yhat,on='Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key.drop('Page',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key.to_csv('Submissions/gbm1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
